{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your vocabulary through tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab = sorted(set(sentence.split()))\n",
    "onehotwords = np.zeros((len(sentence.split()), len(vocab)), int)\n",
    "for i, word in enumerate(sentence.split()):\n",
    "    onehotwords[i, vocab.index(word)] = 1\n",
    "\n",
    "print(' '.join(vocab))\n",
    "onehotwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bow = {}\n",
    "for token in sentence.split():\n",
    "    sentence_bow[token] = 1\n",
    "sorted(sentence_bow.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in sentence.split()])), columns=['sent']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"Construction was done mostly by local masons and carpenters.\\n\" \\\n",
    "            \"He moved into the South Pavilion in 1770.\\n\" \\\n",
    "            \"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\n",
    "corpus = {}\n",
    "corpus['sent0'] = dict((tok.strip('.'), 1) for tok in sentence.split())\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i + 1)] = dict((tok, 1) for tok in sent.split())\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df[df.columns[:7]]  # show just the first 7 tokens (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T\n",
    "df.sent0.dot(df.sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sent0.dot(df.sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sent0.dot(df.sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v) for (k, v) in (df.sent0 & df.sent3).items() if v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Token Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "tokens = re.split(r\"([-\\s.,;!?])+\", sentence)\n",
    "list(filter(lambda x: x if x not in '- \\t\\n.,;!?' else None, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in tokens if x != '' and x not in '- \\t\\n.,;!?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "tokens[-10:]  # just the last 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "tokens = pattern.split(sentence)\n",
    "[x for x in tokens if x != '' and x not in '- \\t\\n.,;!?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
